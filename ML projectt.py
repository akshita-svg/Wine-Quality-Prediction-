# -*- coding: utf-8 -*-
"""ML.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/12_j38O3omriVeMPla8ygOEqYwJL7tqSP

# **WINE QUALITY PREDICTION:**
**The primary goal of using machine learning for wine quality prediction is to develop a model that can accurately predict the quality of wine based on its chemical properties.**

**The dataset used for this analysis was obtained from kaggle . It comprises a collection measurements related to Red wine**

---

### **Import Libraries**
"""

import pandas as pd # data processing , csv files
import matplotlib.pyplot as plt #plotting
import seaborn as sns
import numpy as np

"""## **1.Load Dataset**"""

df=pd.read_csv('/content/winequality-red.csv')
df

"""## **2. Initial Data Exploration**"""

df.head() # Top 5 rows

df.tail()

df.info() # Information about the dataset

df.columns # List of columns

df.shape # Rows and columns

# Statistical summary
df.describe()

"""## **3. Data Cleaning**"""

# Check missing values
df.isnull().sum()

# Check data type.
df.dtypes

df.loc[336]

"""---

## **4. Data Analysis and Visualization**

### **High quality value --> better quality of wine**
"""

df['quality'].value_counts()

sns.set() # polished look for visualization

df.quality.hist() # shows frequency of each unique wine quality rating.
plt.xlabel('Wine Quality')
plt.ylabel('Count')

# Overall Feature Distribution:
df.hist(bins=50,figsize=(15,15))
plt.show()

"""## **Create test dataset (Initial)**"""

from sklearn.model_selection import train_test_split

train_set, test_set = train_test_split(data, test_size=0.2, random_state=42)

"""## **Stratified sampling**
**Data distribution may not be uniform in real world data.
Random sampling - by its nature - introduces biases in such data sets.**
"""

df.quality.hist()
plt.xlabel('Wine Quality')
plt.ylabel('Count')

"""**Many examples of class 5 and 6 compared to the other classes.
This causes a problem while random sampling. The test distribution may not match with the overall distribution.**
"""

from sklearn.model_selection import StratifiedShuffleSplit
sss = StratifiedShuffleSplit(n_splits=1, test_size=0.2, random_state=42) # generates indices for stratified training and test sets.
for train_index, test_index in sss.split(df, df["quality"]):
  strat_train_set = df.loc[train_index]
  strat_test_set = df.loc[test_index]

train_index, test_index= next(sss.split(df, df["quality"]))
strat_train_set = df.loc[train_index]
strat_test_set = df.loc[test_index]

"""### **Distributed comparision**"""

strat_train_set

"""**Let's examine the test set distribution by the wine quality that was used for stratified sampling.**"""

strat_dist = strat_test_set["quality"].value_counts() / len(strat_test_set)

"""**Now compare this with the overall distribution:**"""

overall_dist = df["quality"].value_counts() / len(df)

dist_comparison = pd.DataFrame({'overall': overall_dist, 'stratified': strat_dist})
dist_comparison['diff(s-o)'] = dist_comparison['stratified'] - dist_comparison['overall']
dist_comparison['diff(s-o)_pct'] = 100*(dist_comparison['diff(s-o)']/dist_comparison['overall'])

dist_comparison

"""**Let's contrast this with random sampling**"""

random_dist = test_set["quality"].value_counts() / len(test_set)
random_dist

dist_comparison['random'] = random_dist
dist_comparison['diff(r-o)'] = dist_comparison['random'] - dist_comparison['overall']
dist_comparison['diff(r-o)_pct'] = 100*(dist_comparison['diff(r-o)']/dist_comparison['overall'])

"""## **Sampling bias comparison**
**Compare the difference in distribution of stratified and uniform sampling:**

**Stratified sampling gives us test distribution closer to the overall distribution than the random sampling.**
"""

dist_comparison.loc[:, ['diff(s-o)_pct', 'diff(r-o)_pct']]

"""## **Data visualization** **performed on training set.**"""

exploration_set = strat_train_set.copy()

sns.scatterplot(x='fixed acidity', y='density', hue='quality',
                data=exploration_set)

exploration_set.plot(kind='scatter', x='fixed acidity', y='density', alpha=0.5,
                     c="quality", cmap=plt.get_cmap("jet"))

"""## **Correlation Matrix**"""

corr_matrix = exploration_set.corr()

corr_matrix['quality']

"""**Notice that quality has strong positive correlation with alcohol content [0.48] and strong negative correlation with volitile acidity [-0.38].**"""

plt.figure(figsize=(14,7))
sns.heatmap(corr_matrix, annot=True)

import warnings

# Suppress specific warnings
warnings.filterwarnings("ignore", category=FutureWarning, module="seaborn")

attribute_list = ['citric acid', 'pH', 'alcohol', 'sulphates', 'quality']
sns.pairplot(exploration_set[attribute_list])
plt.show()

from pandas.plotting import scatter_matrix
scatter_matrix(exploration_set[attribute_list])
plt.show()

"""**Separate features and labels from the training set.**"""

wine_features = strat_train_set.drop("quality", axis=1)

wine_labels = strat_train_set['quality'].copy()

wine_features.isna().sum()

from sklearn.impute import SimpleImputer
imputer = SimpleImputer(strategy="median")

imputer.fit(wine_features)

imputer.statistics_

"""Note that these are median values for each feature. We can cross-check it by calculating median on the feature set:"""

wine_features.median()

tr_features = imputer.transform(wine_features)

tr_features.shape

"""**Converting categories to numbers**"""

from sklearn.preprocessing import OrdinalEncoder
ordinal_encoder = OrdinalEncoder()

from sklearn.preprocessing import OneHotEncoder
cat_encoder = OneHotEncoder()

"""## **Data Preprocessing(Transformation Pipeline)**"""

from sklearn.pipeline import Pipeline
from sklearn.preprocessing import StandardScaler

transform_pipeline = Pipeline([
                               ('imputer', SimpleImputer(strategy="median")),
                               ('std_scaler', StandardScaler()),])
transform_pipeline

wine_features_tr = transform_pipeline.fit_transform(wine_features)

"""### **Select and train model**

## **Linear Regression:**
"""

from sklearn.linear_model import LinearRegression

lin_reg = LinearRegression()
lin_reg.fit(wine_features_tr, wine_labels)

from sklearn.metrics import mean_squared_error

quality_predictions =  lin_reg.predict(wine_features_tr)
mean_squared_error(wine_labels, quality_predictions)

"""---

## **Evaluating performance on test set**
"""

wine_features_test = strat_test_set.drop("quality", axis=1)

wine_labels_test = strat_test_set['quality'].copy()

wine_features_test_tr = transform_pipeline.transform(wine_features_test)

quality_test_predictions = lin_reg.predict(wine_features_test_tr)
mean_squared_error(wine_labels_test, quality_test_predictions)

plt.scatter(wine_labels_test, quality_test_predictions)
plt.plot(wine_labels_test, wine_labels_test, 'r-')
plt.xlabel('Actual quality')
plt.ylabel('Predicted quality')

"""**The model seem to be making errors on the best and poor quality wines.**

## **Decision Tree Regressor**
"""

from sklearn.tree import DecisionTreeRegressor

tree_reg = DecisionTreeRegressor()
tree_reg.fit(wine_features_tr, wine_labels)

quality_predictions =  tree_reg.predict(wine_features_tr)
mean_squared_error(wine_labels, quality_predictions)

quality_test_predictions = tree_reg.predict(wine_features_test_tr)
mean_squared_error(wine_labels_test, quality_test_predictions)

"""**Note that the training error is 0, while the test error is 0.58. This is an example of an overfitted model.**


"""

plt.scatter(wine_labels_test, quality_test_predictions)
plt.plot(wine_labels_test, wine_labels_test, 'r-')
plt.xlabel('Actual quality')
plt.ylabel('Predicted quality')

"""### **Cross Validation Score**"""

from sklearn.model_selection import cross_val_score

def display_scores(scores):
  print("Scores:", scores)
  print("Mean:", scores.mean())
  print("Standard deviation:", scores.std())

scores = cross_val_score(lin_reg, wine_features_tr, wine_labels,
                         scoring="neg_mean_squared_error", cv=10)
lin_reg_mse_scores = -scores
display_scores(lin_reg_mse_scores)

scores = cross_val_score(tree_reg, wine_features_tr, wine_labels,
                         scoring="neg_mean_squared_error", cv=10)
tree_mse_scores = -scores
display_scores(tree_mse_scores)

"""---

## **Random Forest Regressor**
"""

from sklearn.ensemble import RandomForestRegressor

forest_reg = RandomForestRegressor()
forest_reg.fit(wine_features_tr, wine_labels)

scores = cross_val_score(forest_reg, wine_features_tr, wine_labels,
                         scoring="neg_mean_squared_error", cv=10)
forest_mse_scores = -scores
display_scores(forest_mse_scores)

quality_test_predictions = forest_reg.predict(wine_features_test_tr)
mean_squared_error(wine_labels_test, quality_test_predictions)

plt.scatter(wine_labels_test, quality_test_predictions)
plt.plot(wine_labels_test, wine_labels_test, 'r-')
plt.xlabel('Actual quality')
plt.ylabel('Predicted quality')

"""**Random forest looks more promising than the other two**

---

## **Fine-tuning the models**
"""

from sklearn.model_selection import GridSearchCV

param_grid = [
 {'n_estimators': [3, 10, 30], 'max_features': [2, 4, 6, 8]},
 {'bootstrap': [False], 'n_estimators': [3, 10], 'max_features': [2, 3, 4]},
]

grid_search = GridSearchCV(forest_reg, param_grid, cv=5,
                           scoring='neg_mean_squared_error',
                           return_train_score=True)

grid_search.fit(wine_features_tr, wine_labels)

grid_search.best_params_

cvres = grid_search.cv_results_
for mean_score, params in zip(cvres["mean_test_score"], cvres["params"]):
  print(-mean_score, params)

grid_search.best_estimator_

"""---

## **Evaluation on test set**

**Now that we have a reasonable model, we evaluate its performance on the test set.**
"""

wine_features_test = strat_test_set.drop("quality", axis=1)

wine_labels_test = strat_test_set['quality'].copy()

wine_features_test_tr = transform_pipeline.transform(wine_features_test)

quality_test_predictions = grid_search.best_estimator_.predict(
    wine_features_test_tr)

mean_squared_error(wine_labels_test, quality_test_predictions)

"""**It's a good idea to get 95% confidence interval of the evaluation metric. It can be obtained by the following code**


"""

from scipy import stats
confidence = 0.95
squared_errors = (quality_test_predictions - wine_labels_test) ** 2
stats.t.interval(confidence, len(squared_errors) - 1,
                 loc=squared_errors.mean(),
                 scale=stats.sem(squared_errors))

"""#**CONCLUSION**

**The Random Forest Classifier demonstrated the highest accuracy among all the evaluated models, indicating its strong performance in correctly predicting the target variable overall. This result highlights its reliability as an ensemble learning method, leveraging multiple decision trees to enhance predictive power and reduce overfitting.**

**On the other hand, the Decision Tree Classifier, while recording the lowest accuracy compared to other models, showed an interesting characteristic—it achieved the highest number of correct predictions after the Gradient Bossting Classifier.**

**This finding is essential for understanding model behavior beyond basic accuracy metrics.The outcome highlights the importance of considering multiple evaluations, such as Confusion Matrix, to fully understand a model’s utility in various contexts.**
"""